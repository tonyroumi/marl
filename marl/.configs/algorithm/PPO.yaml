name: PPO
actors: ["agent_0"]
critics: ["agent_0"]
##IMPORTANT MAKE SURE CORRESPONDING ACTORS AND CRITICS ARE AT THE SAME INDEX
#For single actor-critic networks, do the same.

global:
  num_learning_epochs: 1
  num_mini_batches: 1
  clip_param: 0.2
  gamma: 0.99
  lambda_: 0.95
  value_loss_coef: 1.0
  entropy_coef: 0.0
  learning_rate: 1e-3
  max_grad_norm: 1.0
  use_clipped_value_loss: true
  schedule: "fixed"
  desired_kl: 0.01
  normalize_advantage_per_mini_batch: false
  total_timesteps: 1_000_000

# agent_specific_hyperparams: 
#   actor_0:
#     learning_rate: 1e-4
#   critic_0:
#     learning_rate: 1e-4
